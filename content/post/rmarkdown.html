---
title: "Bayesian moderation analysis"
author: "Wiktor Soral"
slug: test
categories: []
tags: ['Bayesian']
date: "7 02 2019"
---



<p><em>This post is the first of a series of short tutorials on how to approach statistical analyses common among social psychologist using Bayesian approach and R package - brms</em></p>
<p>Moderation analysis is one the most widely used tools to approach interactions in regression models with at least 1 continuous term. Its popularity has outbursted since the release of an SPSS/SAS macro <a href="https://www.processmacro.org/">PROCESS</a>. PROCESS is easy to use and allows to fit a variety of models. However, it lacks certain capabilities (like ability to fit multilevel models or generalized regression models or mix of both). Also PROCESS cannot handle Bayesian moderation analysis. One can use <a href="https://jasp-stats.org/">JASP</a> to conduct Baysian regression analysis with interaction terms, but sadly it’s post-processing capabilities are quite limited (at least to me).</p>
<p>In this post I will show, how to easily and in just a few steps fit Bayesian moderation model in R. As some of my colleagues admitted lack of R package that would replace PROCESS is one of the major factors that restrains them from swapping SPSS with R. Actually there are certain packages that aspire to replace PROCESS in R. Here, however I would to show you that you don’t need to use a specialized package and you can go on with a quite general one - <code>brms</code>.</p>
<p>Lets start with something relatively simple and easy to follow.</p>
<div id="continuous-and-categorical-predictors" class="section level2">
<h2>Continuous and categorical predictors</h2>
<p>In this post I will extensively use <code>tidyverse</code>.</p>
<pre class="r"><code># install.packages(&quot;tidyverse&quot;)
# install.packages(&quot;carData&quot;)
library(tidyverse)
library(carData) # for an example dataset

data(Moore)

Moore %&gt;% 
  glimpse()</code></pre>
<pre><code>## Observations: 45
## Variables: 4
## $ partner.status &lt;fct&gt; low, low, low, low, low, low, low, low, low, low,…
## $ conformity     &lt;int&gt; 8, 4, 8, 7, 10, 6, 12, 4, 13, 12, 4, 13, 7, 9, 9,…
## $ fcategory      &lt;fct&gt; low, high, high, low, low, low, medium, medium, l…
## $ fscore         &lt;int&gt; 37, 57, 65, 20, 36, 18, 51, 44, 31, 36, 42, 56, 2…</code></pre>
<p>By default contrast are set to <code>contr.treatment</code>. This means that a <span class="math inline">\(j\)</span> regression coefficient a categorical variable is interpreted as a difference of means between a level <span class="math inline">\(j+1\)</span> of the variable and a reference level <span class="math inline">\(1\)</span>.</p>
<pre class="r"><code>contrasts(Moore$partner.status)</code></pre>
<pre><code>##      low
## high   0
## low    1</code></pre>
<p>We switch contrasts to <code>contr.sum</code> for the categorical variable.</p>
<pre class="r"><code>contrasts(Moore$partner.status) &lt;- &quot;contr.sum&quot;
contrasts(Moore$partner.status)</code></pre>
<pre><code>##      [,1]
## high    1
## low    -1</code></pre>
<p>We standardize the continuous variable - fscore.</p>
<pre class="r"><code>Moore %&gt;% 
  mutate(fscore_c = (fscore - mean(fscore))/sd(fscore)) -&gt; Moore</code></pre>
<p>We fit our model with <code>brms</code> package. Both installation and fitting may take a while.</p>
<pre class="r"><code>library(brms)
theme_set(theme_default())</code></pre>
<p>We will use weakly informative priors for regression coefficients. By defaults <code>brms</code> uses robust Student-t prior for model error. <em>Robust</em> means here robust to potential outliers.</p>
<pre class="r"><code>prior &lt;- prior(normal(0, 10), class = b)</code></pre>
<p>Fitting is quite straigthforward. If your run your model on your laptop with less than 4 cores, it is better to change the value of <code>cores</code> to 2 or even 1.</p>
<pre class="r"><code># install.packages(&quot;brms&quot;)

fit1 &lt;- 
  brm(data = Moore,
      conformity ~ partner.status * fscore_c, ## means partner.status + fscore_c + partner.status:fscore_c
      prior = prior, sample_prior = T,
      chains = 4, cores = 4)</code></pre>
<p>We can start with printing summary of posterior distribution of our model’s coefficients.</p>
<pre class="r"><code>fit1 %&gt;% 
  posterior_summary() %&gt;% 
  knitr::kable(digits = 2)</code></pre>
<table>
<thead>
<tr class="header">
<th></th>
<th align="right">Estimate</th>
<th align="right">Est.Error</th>
<th align="right">Q2.5</th>
<th align="right">Q97.5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>b_Intercept</td>
<td align="right">12.14</td>
<td align="right">0.71</td>
<td align="right">10.71</td>
<td align="right">13.58</td>
</tr>
<tr class="even">
<td>b_partner.status1</td>
<td align="right">2.12</td>
<td align="right">0.72</td>
<td align="right">0.74</td>
<td align="right">3.52</td>
</tr>
<tr class="odd">
<td>b_fscore_c</td>
<td align="right">-0.29</td>
<td align="right">0.72</td>
<td align="right">-1.73</td>
<td align="right">1.12</td>
</tr>
<tr class="even">
<td>b_partner.status1:fscore_c</td>
<td align="right">-1.85</td>
<td align="right">0.72</td>
<td align="right">-3.25</td>
<td align="right">-0.41</td>
</tr>
<tr class="odd">
<td>sigma</td>
<td align="right">4.70</td>
<td align="right">0.55</td>
<td align="right">3.78</td>
<td align="right">5.95</td>
</tr>
<tr class="even">
<td>prior_b</td>
<td align="right">-0.11</td>
<td align="right">9.85</td>
<td align="right">-19.02</td>
<td align="right">19.07</td>
</tr>
<tr class="odd">
<td>prior_sigma</td>
<td align="right">11.33</td>
<td align="right">13.44</td>
<td align="right">0.36</td>
<td align="right">44.93</td>
</tr>
<tr class="even">
<td>lp__</td>
<td align="right">-147.04</td>
<td align="right">1.75</td>
<td align="right">-151.48</td>
<td align="right">-144.79</td>
</tr>
</tbody>
</table>
<p>Compare it to our prior distribution.</p>
<pre class="r"><code>prior_summary(fit1)</code></pre>
<pre><code>##                  prior     class                     coef group resp dpar
## 1        normal(0, 10)         b                                         
## 2                              b                 fscore_c                
## 3                              b          partner.status1                
## 4                              b partner.status1:fscore_c                
## 5 student_t(3, 12, 10) Intercept                                         
## 6  student_t(3, 0, 10)     sigma                                         
##   nlpar bound
## 1            
## 2            
## 3            
## 4            
## 5            
## 6</code></pre>
<p>Here we print simple slopes.</p>
<pre class="r"><code>marginal_effects(fit1, &quot;partner.status:fscore_c&quot;)</code></pre>
<p><img src="/post/rmarkdown_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code>marginal_effects(fit1, &quot;fscore_c:partner.status&quot;)</code></pre>
<p><img src="/post/rmarkdown_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>We print the results of simple slopes analysis.</p>
<pre class="r"><code>hypothesis(fit1, c(highF=&quot;partner.status1 + partner.status1:fscore_c = 0&quot;,
                   mediumF=&quot;partner.status1 = 0&quot;,
                   lowF=&quot;partner.status1 - partner.status1:fscore_c = 0&quot;))</code></pre>
<pre><code>## Hypothesis Tests for class b:
##   Hypothesis Estimate Est.Error CI.Lower CI.Upper Evid.Ratio Post.Prob
## 1      highF     0.27      1.01    -1.73     2.30      13.80      0.93
## 2    mediumF     2.12      0.72     0.74     3.52       0.20      0.16
## 3       lowF     3.98      1.02     1.99     5.98       0.01      0.01
##   Star
## 1     
## 2    *
## 3    *
## ---
## &#39;*&#39;: The expected value under the hypothesis lies outside the 95%-CI.
## Posterior probabilities of point hypotheses assume equal prior probabilities.</code></pre>
</div>
